---
title: "stat6021week4"
author: "Kurt"
date: "2024-08-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Monday - Logistic Regression
The logistic regression (logit model) is used to model the probability of a dichotomous outcome variable. For instance, if we are interested in
building a model to predict:
• The probability of a win/loss in an NBA game based on data on free throw percentage, number of rebounds, steals, blocks, etc.
• The probability of a political candidate winning/losing an election based on polling data, amount of money spent on the campaign, the amount of time spent campaigning negatively and whether or not the candidate is an incumbent.
• The probability of whether an incoming email needs to be classified as spam or not based on sender of the email and number of typos in the email.
• The probability of approval/non-approval of loan for a bank customer based on credit score, income, number of credit cards, etc.
• The probability of a particular cancer based on patient comorbidities.
n the logit model, the log odds of the outcome is modeled as a linear
combination of the predictor variables.
• More formally, let Y be the binary outcome variable indicating success/failure with probability of success p. That is, P(Y = 1) = p.
Then the logistic regression of Y on the predictor variables
x1, x2, · · · , xk is
logit(p) = ln  (p/(1 − p)) = β0 + β1x1 + β2x2 + · · · + βk xk

Interpreting the slope coefficients
Logistic Regression slope coefficients are interpreted the same as multiple regression slope coefficients, except that the response variable in the modelis in terms of the log of the odds. If we exponentiate the coefficients, we can interpret them as the change in the odds of the response for every one unit increase in the predictor, holding other predictors fixed.
How much change in the odds of the response variable:
• If the exponentiated coefficient is between 0 and 1, there is a (1−exponentiated coefficient) ×100% decrease in the odds of the response for every one unit increase in the corresponding predictor, holding other predictors fixed.
• If the exponentiated coefficient is above 1, there is a (exponentiated coefficient −1) × 100% increase in the odds of the response for every one unit increase in the corresponding predictor, holding other predictors fixed.
• If the exponentiated coefficient is exactly 1, there is no change in the odds of the response for every one unit increase in the corresponding predictor, holding other predictors fixed.



```{r}
nba <- read.csv("NBA_raw.csv")
View(nba)
```
```{r}
library(tidyverse)
```
```{r}
nba2<- nba %>%
  mutate(home_away=ifelse(grepl('vs',MATCHUP), 'home', 'away'),
         win=ifelse(W.L=='W',1,0)) %>%
  rename(FGP=FG.,TPP=X3P.,FTP=FT.) %>%
  select(W.L,win,home_away,MIN,PTS,FGP,TPP,FTP,REB,AST,STL,BLK,TOV,PF)
View(nba2)
```
```{r}
ggplot(nba2, aes(home_away,fill= factor(win), colour = factor(win))) + geom_bar(position = 'fill')
dat <-nba2[,-c(1,2,3)]
cor_mat <- round(cor(dat), 2)
ggcorrplot::ggcorrplot(cor_mat, lab = T, method = 'circle', type = 'lower')
```
```{r}
logit_model2 <- glm(win~.-W.L, nba2, family='binomial')
summary(logit_model2)
```

```{r}
library(caret)
logit_model1 <- glm(win~.-W.L-PTS, nba2, family='binomial')
logit_model3 <- train(W.L~.-win, nba2, method = 'glm', family = 'binomial')
summary(logit_model3)
```
```{r}
logit_model4 <- train(W.L~.-win-PTS,nba2, method = 'glm', family = 'binomial')
summary(logit_model4)
```

```{r}
aic<-MASS::stepAIC(logit_model2, direction = "both", trace=FALSE)
summary(aic)
#OR in caret train()

logit_aic<-train(W.L~.-win-PTS,nba2,method="glmStepAIC",family="binomial", trace=FALSE)
summary(logit_aic)

# Note same results to logit_model 3 and 4.

## Idea two: interact home_away with PTS since home_away 
#might affect PTS

logit_model5<-train(W.L~.-win+PTS*home_away,nba2,method="glm",family="binomial")
summary(logit_model5)

#OR

logit_model6<-train(W.L~.-win+PTS*home_away,nba2,method="glm",family="binomial")
summary(logit_model6)

# It appears this is not the case since p-values for PTS and 
# the interaction term is still high. Thus, removing PTS from
# the model is a better idea. Although, you can try other 
# interaction ideas, for example FGP (Field Goal Percentage)
# and home_away.
```

```{r}

### Final model for prediction: either logit_model 3 or 4
## or aic or logit aic.

## Let's check for multicoliearity to make compliment the corplot

car::vif(logit_model1)

##OR for the train models:

car::vif(logit_model4$finalModel)

#All vif values are all less tha 5, so no multicolinearity!

### Understanding the model: show how to write down the model
#aic or model 3 or 4 down:

# Ln(p/(1-p)) = -24.499808+0.129650*I_home -0.039539*MIN + ...


## Interpreting the coefficients: refer to slide.
# For example let's interpret coefficient of FGP: 0.375232
# First, exponentiate the coefficients to be able to 
# interpret them as change is odds of a win:

exp(coef(logit_model1))

# Now we will interpret the exponentiated coefficient for 
# FGP: 1.443548e+00 or 1.443548. Note away games is the reference
# category so here is the interpretation:
# For every one percentage point increase in the Field goal 
# percentage for NBA games, the odds of a win increases
# by 44.35%(that is, 1.443548-1=44.35%) for away games.
```

```{r}

## Lastly if you have time, we want to make predictions with
# the model.

# We need new data:

new_dat<-nba2[1:2, -2] 

#Note: it still want a value for W.L and PTS since our model:
# logit_model3<-glm(win~.-W.L-PTS,nba2,family="binomial")
# had -W.L and -PTS. These values are not included
# in the prediction however. You can avoid it by
# subsetting nba2 to remove W.L and PTS and use the 
# resulting dataframe to build the model.

predict(logit_model3,new_dat,type = "response")

##OR

new_dat2<-nba2[1:2, -1]
predict(logit_model4,new_dat2,type = "raw")
predict(logit_model4,new_dat2,type = "prob")

# Two predictions here since we used rows 1 and 2 in nba2.
# Win prob for row 1 is 0.601874707 (Win since >0.5)
# Win prob for row 2 is 0.007856602 (Loss) Which matches the 
# observed outcomes.

# Note: They might ask you about cross validation and confusion
# matrix, I will cover that on Tuesday!

logit_model5 <- train(W.L ~.-win-PTS*home_away,nba2,method='glm', family ='binomial')
summary(logit_model5)
```

Tuesday

```{r}
glm_model <- glm(win~.-W.L-PTS,nba2, family = 'binomial')
logit<- train(W.L~.-win-PTS,nba2,method='glm', family = 'binomial')
exp(coef(glm_model))
```
 For every 1 percaentage point increase in FGP the odds of a win increases by 44.3% (1.443 - 1)
 For home games compared to away games 
exp 0-1 is a decrese in odds, 1 is no change in odds, > 1 is an increase in odds.
exp coef - 1 is how much the odds increase FGP odds increase by .44
1- exp coef  is how much the odds decrease

```{r}
new_dat<-nba2[1:2, -2] 
predict(glm_model,new_dat,type = "response")
#Note: it still want a value for W.L and PTS since our model:
# logit_model3<-glm(win~.-W.L-PTS,nba2,family="binomial")
# had -W.L and -PTS. These values are not included
# in the prediction however. You can avoid it by
# subsetting nba2 to remove W.L and PTS and use the 
# resulting dataframe to build the model
```


```{r}
new_dat2<-nba2[1:2, -1]
predict(glm_model,new_dat2,type = "raw")
predict(glm_model,new_dat2,type = "prob")

```

