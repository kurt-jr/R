---
title: "stat6021week3"
author: "Kurt"
date: "2024-07-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(tidyverse)
insurance <- read.csv("insurance.csv")
head(insurance)
```
```{r}
mod1 <- lm(charges~smoker, data = insurance)
summary(mod1)
```

```{r}
t.test(charges~smoker, data = insurance)
```
```{r}
new <-  data.frame(smoker='yes')
predict(mod1, new)
```
```{r}
mod2 <- lm(charges~region, data = insurance)
summary(mod2)
```
average insurance cost region northwest is the intercept, each estimate shows the difference between northwest and that region.


```{r}
insurance$region2 <- factor(insurance$region, levels = c("southeast","northeast", "northwest", "southwest"))
mod3 <- lm(charges~region2, data = insurance)
summary(mod3)
```
Anova:
H0: µne = µnw = µse = µsw
HA: At least one mean is different
```{r}
anov <- aov(charges~region, insurance)
summary(anov)
TukeyHSD(anov)
```


```{r}
ggplot(insurance, aes(age,charges, colour = smoker)) + geom_jitter() + geom_smooth(method = "lm", se = F, aes(group = smoker))
```
```{r}
mod4 <- lm(charges~ age+smoker, data = insurance)
summary(mod4)
```
intercept age = 0 cost = -2391.63
average  nonsmoker insurance cost increases by 274.87
average  smoker insurance cost increases by 23855.30
This is a **no-interaction model**
A regression model that includes a dummy variable for a categorical
predictor allows for different intercepts but not different slopes for the
two groups. This is called a **no-interaction model**, because it
assumes that the effect of the numerical predictor on the response is
the same for both groups



A more general model allows for different slopes as well as different
intercepts. An **interaction model** allows for the effect of a numerical
predictor to differ between the two/more groups.
* allows for interaction in lm function
```{r}
mod5int <- lm(charges~ age*smoker, data = insurance)
summary(mod5int)
```

as age increases by 1 the cost of insurance increases by 37.99
```{r}
new_dat<-data.frame(age=26, smoker="no")
predict(mod5int, new_dat, interval = "prediction")
```

```{r}
ins<-insurance[,-8]
mod10 <- lm(charges ~., data = ins)
summary(mod10)
```
```{r}
install.packages("broom")
library(broom)
dat_ins <- mod5int %>% augment(ins)
view(dat_ins)
filter(dat_ins, .std.resid>2)
```

Wednesday- outliers
treating outliers
Transform response
2 Consider context and subject matter knowledge
3 Measurement error?
4 Winsorize: Replace extreme values with the nearest values that are
not outliers. For example, replace values above the 95th percentile
with the value at the 95th percentile.
5 Trim/remove point (last resort)
6 Use other techniques like quantile regression or Random Forest which
are resistant to outliers

Leverage Points: data points that have extreme values for the predictor
variables. These points have a large influence on the fitted values of the
model but not necessarily on the regression coefficients themselves.
Detection:
1 Leverage values are in the diagonals of the Hat matrix: (XT X)−1XT
2 High leverage points are identified by leverage values greater than 2(p+1)/n


Influential points are data points that have a disproportionately large
impact on the regression model’s coefficients and fitted values.
1 NOTE: Data points that are both high leverage points and outliers
are often influential because their combination of having extreme
predictor values and large residuals gives them a significant impact on
the regression model
Detection: Cook’s Distance: a threshold of 4/n

```{r}
model1 <- lm(charges~., data = insurance)
summary(model1)
```
```{r}
diagnostics <- model1 %>%
  augment(data = insurance)
outliers <- filter(diagnostics, abs(.std.resid) >3)
leverage <- filter(diagnostics, .hat >2*(6+1)/nrow(insurance))
influence <- filter(diagnostics, .cooksd>4/nrow(insurance))
head(influence)
head(leverage)
```
.hat is leverage point


train-test split
do not replace, not replacing by default
```{r}
split <- sample(1:nrow(insurance), size = floor(0.8*nrow(insurance)))
train_data <- insurance[split,]
test_data <- insurance[-split,]
model2 <- lm(charges~age+bmi+children+smoker+region, data = train_data)
summary(model2)
predictions <- predict(model2,test_data)
rmse<-sqrt(mean((predictions-test_data$charges)^2))
rmse
RMSE(predictions, test_data$charges)
R2(predictions, test_data$charges)
MAE(predictions, test_data$charges)
```

```{r}
install.packages("caret")
library(caret)
map_dbl(1:5, ~.x^2)
CV <- function(data,k){
  folds<-createFolds(insurance$charges,k=k)
  map_dbl(folds,function(indices){
    train_data<-data[-indices,]
    test_data<-data[indices,]
    model5 <- lm(charges~.-sex,data=train_data)
    predictions<-predict(model5, test_data)
    sqrt(mean((predictions-test_data$charges)^2))
  }) %>% mean()
}
CV(insurance, 5)

repeated <- replicate(10,CV(insurance, 5))
mean(repeated)
```

Thursday
```{r}
model0 <- lm(charges~.-sex, data = insurance)
model1 <- train(charges~.-sex,method = "lm" ,data = insurance)

control <- trainControl(method = "cv", number = 5, repeats = 10)
model2 <- train(charges~.-sex,method = "lm" ,trControl = control, data = insurance)
summary(model2)
model2$results$RMSE

```


Ridge/ Lasso Regression
Generally the Mean Square Error (MSE) of an estimator, ˆβ is:
MSE(ˆβ) = (Bias(ˆβ))2 + Var (ˆβ)
1. We typically cannot achieve both low Bias and low variance simultaneously (which leads to bias-variance tradeoff)
2. Shrinkage methods adds an amount of smart bias to reduce variance. Hence enhancing interpretations of the bias versions of the estimator.
3. The term ”shrinkage” typically refers to methods that pull estimates towards a central value, often zero. This can help improve the model’s performance on new data by preventing overfitting.
Ridge and Lasso (least absolute shrinkage and selection operator) are Shrinkage techniques.

hrinkage techniques enforce sparsity in the estimates is by minimizing the
Residual Sum of Squares (RSS) plus a penalty term that favors sparsity on
the estimated coefficients. Example:
- Ridge regression: enforces a quadratic penalty to the candidate slope coefficients and seek to minimize
RSS(ˆβ) + λ∑β^2

- Lasso regression: enforces a absolute value penalty to the candidate slope coefficients and seek to minimize
RSS(ˆβ) + λ∑ |β|
where λ is the penalty/tunning parameter.
Elastic net combines Ridge and Lasso:
RSS(ˆβ) + λ [∝∑β^2 + ∝∑|β|]

```{r}
laliga <- read.csv("laliga.csv")
View(laliga)
laliga2 <- laliga[,-c(1,3,9)]
modellaliga <- lm(Points~.,data = laliga2)
aic <- MASS::stepAIC(modellaliga, direction = "both", trace = F)
summary(aic)
car::vif(aic$model)
library(glmnet)
x <-model.matrix(Points~0+.,data = laliga2)
head(x)
y <- laliga2$Points
rmodel <- glmnet(x,y, alpha = 0)
plot(rmodel, label = T, xvar = "lambda")
plot(rmodel, label = T, xvar = "dev")
kcvglmnet <- cv.glmnet(x,y, alpha = 0, nfolds = 3)
kcvglmnet$lambda.min
predict(rmodel, type = "response", s=kcvglmnet$lambda.min,newx=x[1:2,])
```


Friday

```{r}
x <-model.matrix(Points~0+.,data = laliga2)
y <- laliga2$Points
rmodel <- glmnet(x,y, alpha = 0)
kcvglmnet <- cv.glmnet(x,y, alpha = 0, nfolds = 3)
kcvglmnet$lambda.min
lse <- kcvglmnet$lambda.lse
lse
```

```{r}
predict(rmodel, type = 'response', s=kcvglmnet$lambda.lse,newx = x[1:2,])
predict(rmodel, type = 'coefficient', s=kcvglmnet$lambda.lse,newx = x[1:2,])


plot(rmodel, label=T, xvar = 'lambda')+abline(v=log(kcvglmnet$lambda.lse))
```

Principal Component Analysis (PCA) is a multivariate technique designed to summarize the most important features and relations of p numerical random variables.

1 PCA computes a new set of variables, the principal component that contains the same information as the p numerical random variables but expressed in a more convenient way.
2 The goal of PCA is to retain only a limited number of principal components that explain most of the information, therefore performing dimension reduction.
3 Remarkably, PCA computes the principal components in an ordered way: the first principal component explains the most of the information (quantified as the variance) of p numerical random variables, and then the explained information decreases monotonically down to the last principal component. That is:
Var (Γ1) ≥ Var (Γ2) ≥ · · · Var (Γp )
4 The principal components are uncorrelated even if the p numerical random variables are correlated.

```{r}
pca <- princomp(laliga2, fix_sign = T)
summary(pca)
plot(pca,type ='l')
pca2<- princomp(laliga2, cor = T, fix_sign = T)
summary(pca2)
pca2$scores
biplot(pca2)
```

The key idea behind Principal Components Regression (PCR) is to regress the response in a set of principal components obtained from the predictors
• The motivation is that often a small number of principal components is enough to explain most of the variability of the predictors and consequently their relationship with the response.
• Advantages: No multicolinearity, less coefficients to estimate
• Disadvantages:
• Interpretation of the coefficients is not directly related with the predictors, but with the principal components. Hence, the interpretability of a given coefficient in the regression model is tied to the interpretability of the associated principal component.
• Prediction needs an extra step, since it is required to obtain the scores of the new observations of the predictors.

```{r}
laliga3<- laliga2[,-1]
pca3 <- princomp(laliga3, cor=T,fix_sign=T)
pca_data<- data.frame(Points= laliga2$Points, pca3$scores)
pcareg<- lm(Points~., data = pca_data)
summary(pcareg)
```

```{r}
car::vif(pcareg)
pcareg2<-lm(Points~Comp.1+Comp.2+Comp.3, data = pca_data)
summary(pcareg2)
```
```{r}
install.packages("pls")
library(pls)
```

```{r}
la14 <-subset(laliga2, select = -c(Wins,Draws,Loses))
pcareg3 <- pcr(Points~., data=la14,scale=T)
summary(pcareg3)
```

